# Study Test Cases in Suite

## Purpose
Analyze and understand the patterns, structure, and conventions of existing test cases within a Test Suite to establish a baseline before creating new test cases. This helps ensure consistency and adherence to the team's testing standards.

## Required Input
- **Test Suite ID** (Azure DevOps Test Suite Work Item ID)

## Process

### Step 1: Retrieve Test Suite Information
Use DevOps MCP server to get the test suite details:
```
mcp_devops-mcp_getWorkItem(id: [Test Suite ID])
```

Extract:
- Suite name and description
- Area Path and Iteration Path
- Owner and current state
- Test Suite Type (Static, Dynamic, Query-based)
- Test case IDs from the audit field (`Microsoft.VSTS.TCM.TestSuiteAudit`)

### Step 2: Retrieve All Test Cases
For each test case ID found in the suite:
```
mcp_devops-mcp_getWorkItem(id: [Test Case ID])
```

Collect the following information from each test case:
- `System.Title` - Test case title
- `System.State` - Current state (Design, Ready, Closed)
- `System.AreaPath` - Area Path
- `System.IterationPath` - Iteration Path
- `Microsoft.VSTS.Common.Priority` - Priority level
- `Microsoft.VSTS.TCM.AutomationStatus` - Automation status
- `Microsoft.VSTS.TCM.Steps` - Test steps in XML format
- `System.AssignedTo` - Current assignee
- `System.CreatedBy` - Original creator
- `System.CreatedDate` - Creation date

### Step 3: Analyze Naming Patterns
Examine all test case titles to identify:

**Common Patterns:**
- Prefix conventions (e.g., "Check", "Verify", "Test")
- Suffix conventions (e.g., " - Mobile", " - Desktop", " - [Browser]")
- Device/platform indicators
- Functional area grouping

**Examples to Extract:**
- Format: "Check [component] - [context]"
- Format: "Verify [action] on [page] - [device]"
- Grouping: Mobile vs Desktop variations
- Naming length and style consistency

### Step 4: Analyze Test Step Structure
For each test case, parse the XML steps and analyze:

**Step Types Distribution:**
- Count of `ValidateStep` vs `ActionStep`
- Typical step count per test case (min, max, average)
- Step ID sequencing patterns

**Content Analysis:**
- Action descriptions (first parameterizedString)
- Expected results (second parameterizedString)
- HTML formatting patterns used
- Use of conditional statements (e.g., "On D2C markets...")
- Level of detail in step descriptions

**Common Action Verbs:**
- Navigation: "Navigate to", "Go to", "Open"
- Interaction: "Click on", "Tap", "Scroll", "Select", "Enter"
- Verification: "Check that", "Verify", "Ensure"

### Step 5: Identify HTML Formatting Conventions
Analyze the HTML structure within test steps:

**Wrapping Elements:**
- `<DIV><P>` for paragraphs
- `<SPAN>` with inline styles for emphasis
- `<BR/>` for line breaks

**Common Styles:**
- Color schemes: `color: rgba(0, 0, 0, 0.9)`
- Background colors: `background-color: rgb(239, 246, 252)`
- Inline vs block formatting

**HTML Encoding:**
- Use of `&lt;` and `&gt;` for angle brackets
- Use of `&amp;` for ampersands
- Proper XML escaping patterns

### Step 6: Extract Test Scope Patterns
Identify what types of testing are covered:

**Functional Categories:**
- UI component verification
- User interactions
- Navigation flows
- State management
- Responsive behavior
- Localization/internationalization
- Market-specific features
- Integration points

**Device/Platform Coverage:**
- Mobile-specific tests
- Desktop-specific tests
- Cross-platform tests
- Browser-specific tests

### Step 7: Identify Common Test Data Patterns
Look for:
- Market/country variations
- Language variations
- User type variations (B2B, D2C, authenticated, guest)
- Environment-specific data (URLs, test accounts)

### Step 8: Document Parameters and Configurations
Check if test cases use:
- Parameterized test data
- Test configurations
- Shared steps
- Prerequisites or setup steps

## Output Summary

Provide a comprehensive analysis report including:

### 1. Suite Overview
- Suite name and ID
- Total test case count
- State distribution (Design, Ready, Closed)
- Date range (oldest to newest)
- Primary contributors

### 2. Naming Conventions
- Standard format(s) identified
- Common prefixes and suffixes
- Device/platform indicators
- Examples of well-formatted titles

### 3. Test Step Patterns
- Average steps per test case
- Step type distribution
- Common action verbs
- Expected result formatting
- Level of detail

### 4. HTML and Formatting Standards
- Standard HTML wrappers
- Common inline styles
- Encoding patterns
- Formatting consistency

### 5. Test Coverage Analysis
- Functional areas covered
- Device/platform distribution
- Test scenario types
- Gaps or underrepresented areas

### 6. Quality Indicators
- Consistency score across test cases
- Completeness of test steps
- Clarity of expected results
- Traceability (links to requirements)

### 7. Recommendations
- Patterns to follow for new test cases
- Areas for improvement
- Suggested naming for new tests
- Step structure recommendations

## Example Output Format

```markdown
## Test Suite Analysis: [Suite Name] (ID: [Suite ID])

### Overview
- **Total Test Cases**: [count]
- **States**: [Design: X, Ready: Y, Closed: Z]
- **Date Range**: [earliest] to [latest]
- **Primary Area**: [Area Path]

### Naming Convention
**Standard Format**: Check [functionality] - [Device]

**Examples**:
- Check the header - mobile
- Check country selector - Mobile
- Check B2B entries - Desktop

### Test Step Structure
- **Average Steps**: 4.5 steps per test case
- **Step Types**: 85% ValidateStep, 15% ActionStep
- **Common Actions**: Navigate, Click, Tap, Check, Verify
- **Expected Results**: Specific, measurable outcomes with conditional logic

### HTML Formatting
```xml
<DIV><P>Action description</P></DIV>
<SPAN style="color: rgba(0, 0, 0, 0.9);background-color: rgb(239, 246, 252)">Text</SPAN>
```

### Coverage
- **Mobile Tests**: 60%
- **Desktop Tests**: 40%
- **Functional Focus**: Header, Navigation, Localization, B2B Features

### Recommendations for New Test Cases
1. Follow "Check [component] - [device]" naming pattern
2. Use 3-6 ValidateStep entries per test case
3. Start with availability checks, then interactions, then state verification
4. Include market-specific conditionals when applicable
5. Maintain HTML formatting consistency with SPAN tags for emphasis
```

## Usage Notes
- Run this analysis before creating new test cases in the same suite
- Use findings to ensure new tests match existing conventions
- Update analysis periodically as test suite evolves
- Reference this analysis when onboarding new team members
- Compare different suites to identify org-wide patterns

## Integration with CreateTestCaseInSuite
This analysis provides the context needed for the CreateTestCaseInSuite prompt:
- Establishes naming conventions to follow
- Defines step structure patterns
- Provides HTML formatting guidelines
- Identifies common test scenarios
- Sets quality benchmarks
